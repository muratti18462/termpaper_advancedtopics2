{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers\n",
    "%pip install -U datasets\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl\n",
    "%pip install -U bitsandbytes\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "\n",
    "    )\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set a reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 1024:\n",
    "    tokenizer.model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruction\n",
    "instruction = (\"Determine whether the following text was generated by a human or an AI. \"\n",
    "               \"If the text is human-generated, generate '1'; if AI-generated, generate '0'. \"\n",
    "               \"Do not provide any explanation, just generate '1' or '0'.\")\n",
    "\n",
    "# Template for formatting the prompt\n",
    "classification_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Text:\n",
    "{}\n",
    "\n",
    "### Label:\n",
    "{}\"\"\"\n",
    "\n",
    "# Function to format each row according to your template\n",
    "def format_prompt_template(example, tokenizer):\n",
    "    formatted_text = classification_prompt.format(instruction, example[\"text\"], example[\"label\"])\n",
    "    example[\"text\"] = formatted_text\n",
    "    return example\n",
    "\n",
    "# Load dataset (adjust the dataset path/source as needed)\n",
    "dataset = load_dataset(\"yaful/MAGE\")\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame for sampling\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Sample a fraction of the dataset\n",
    "sample_fraction = 0.1\n",
    "sample_size = int(len(df) * sample_fraction)\n",
    "min_class_size = int(sample_size / df['label'].nunique())\n",
    "\n",
    "# Sample equally from each class\n",
    "sampled_df = df.groupby('label').apply(lambda x: x.sample(n=min_class_size, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Convert the sampled DataFrame back to a Hugging Face Dataset\n",
    "sampled_train_dataset = Dataset.from_pandas(sampled_df)\n",
    "\n",
    "# Perform train-test split\n",
    "train_test_split = sampled_train_dataset.train_test_split(test_size=0.02, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "validation_dataset = train_test_split['test']\n",
    "\n",
    "column_names = list(train_dataset.features)\n",
    "relevant_columns = [col for col in column_names if col in [\"text\", \"label\"]]\n",
    "\n",
    "# Apply the formatting to both the training and validation datasets\n",
    "train_dataset = train_dataset.map(format_prompt_template,fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                  remove_columns=[col for col in column_names if col not in relevant_columns], num_proc=4)\n",
    "\n",
    "validation_dataset = validation_dataset.map(format_prompt_template,fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                            remove_columns=[col for col in column_names if col not in relevant_columns], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    #num_train_epochs=1,\n",
    "    max_steps=5000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,  \n",
    "    logging_strategy=\"steps\",\n",
    "    warmup_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"./results/checkpoint-5000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if tokenizer.model_max_length > 1024:\n",
    "    tokenizer.model_max_length = 1024\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"yaful/MAGE\")\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Split the test set by label, ensuring equal proportions for each label (0 and 1)\n",
    "df_test_0 = df_test[df_test['label'] == 0]\n",
    "df_test_1 = df_test[df_test['label'] == 1]\n",
    "\n",
    "# Set the desired proportion, e.g., 20% of the test data\n",
    "proportion = 0.1\n",
    "\n",
    "# Sample equal proportion from both labels\n",
    "df_test_0_sample = df_test_0.sample(frac=proportion, random_state=42)\n",
    "df_test_1_sample = df_test_1.sample(frac=proportion, random_state=42)\n",
    "\n",
    "# Combine the two samples\n",
    "df_test_combined = pd.concat([df_test_0_sample, df_test_1_sample]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the result if needed\n",
    "df_test_combined = df_test_combined.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"yaful/MAGE\")\n",
    "\n",
    "# Define a filter function that checks the length of the text\n",
    "def filter_by_length(example):\n",
    "    text_length = len(example['text'])\n",
    "    return 10 <= text_length <= 50\n",
    "\n",
    "# Apply the filter to the 'test' split of the dataset\n",
    "shorter_test_dataset = dataset['test'].filter(filter_by_length)\n",
    "df_short_test = pd.DataFrame(shorter_test_dataset)\n",
    "df_short_test = df_short_test.groupby('label').sample(24, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ood = pd.read_csv(\"/home/ec2-user/data-analysis/ood_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\"Determine whether the following text was generated by a human or an AI. \"\n",
    "               \"If the text is human-generated, generate solely and only label '1'; if AI-generated, generate solely and only label '0'. \"\n",
    "               \"Do not provide any explanation, just generate '1' or '0'. If you were in doubt please provide most likely label.\")\n",
    "\n",
    "classification_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Text:\n",
    "{}\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "label_pattern = r'Label:\\s*(\\d)'\n",
    "\n",
    "def format_prompt_template(text, instruction):\n",
    "    return classification_prompt.format(instruction, text, \"\") \n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    formatted_prompt = format_prompt_template(text, instruction)\n",
    "    print(index)\n",
    "    # Tokenize and make the prediction\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(prediction)\n",
    "    match = re.search(label_pattern, prediction)\n",
    "    if match:\n",
    "        label = match.group(1)  # Extracted '0' or '1'\n",
    "    else:\n",
    "        label = \"N/A\"  # In case no label is found\n",
    "\n",
    "    df.loc[index, \"training_results\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
